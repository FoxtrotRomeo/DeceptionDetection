{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the usual libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sktime.classification.interval_based import CanonicalIntervalForest\n",
    "from sktime.classification.kernel_based import RocketClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the .csv files in merged_openface_out as dataframes in a dictionary\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# get the current directory\n",
    "path = os.getcwd()\n",
    "\n",
    "print(path)\n",
    "# get the path to the directory with the csv files\n",
    "path = path + '/merged_openface_out_cut'\n",
    "# get the list of files in the directory\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "# create an empty dictionary to store the dataframes\n",
    "data = {}\n",
    "# loop through the list of files\n",
    "for filename in all_files:\n",
    "    # get the name of the file\n",
    "    name = os.path.basename(filename)\n",
    "    # delete the .csv extension\n",
    "    name = name[:-4]\n",
    "    # read the file into a dataframe\n",
    "    df = pd.read_csv(filename, index_col='Unnamed: 0', header=0)\n",
    "    # drop the columns starting with timestamp\n",
    "    df = df.drop(df.filter(regex='timestamp').columns, axis=1)\n",
    "    # store the dataframe in the dictionary\n",
    "    data[name] = df\n",
    "\n",
    "# get the path to the directory with the csv files\n",
    "path = os.getcwd()\n",
    "\n",
    "path = path + '/openface_out_A_cut'\n",
    "# get the list of files in the directory\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "print(all_files)\n",
    "# create an empty dictionary to store the dataframes\n",
    "data_A = {}\n",
    "# loop through the list of files\n",
    "for filename in all_files:\n",
    "    # get the name of the file\n",
    "    name = os.path.basename(filename)\n",
    "    # delete the .csv extension\n",
    "    name = name[:-4]\n",
    "    # read the file into a dataframe\n",
    "    df = pd.read_csv(filename, index_col='Unnamed: 0', header=0)\n",
    "    # drop the columns starting with timestamp\n",
    "    df = df.drop(df.filter(regex='timestamp').columns, axis=1)\n",
    "    # store the dataframe in the dictionary\n",
    "    data_A[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of missing values in data and in data_A\n",
    "missing_values = {}\n",
    "for key in data.keys():\n",
    "    missing_values[key] = data[key].isnull().sum().sum()\n",
    "missing_values_A = {}\n",
    "for key in data_A.keys():\n",
    "    missing_values_A[key] = data_A[key].isnull().sum().sum()\n",
    "\n",
    "print(missing_values)\n",
    "print(missing_values_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys = sorted(list(data.keys()))\n",
    "print(sorted_keys)\n",
    "# create a list of groups, where each group is given by the elements of sorted_keys, except the last two characters\n",
    "groups = list(set([key[:-2] for key in sorted_keys]))\n",
    "print(sorted(groups))\n",
    "\n",
    "group_dict = {}\n",
    "for group in groups:\n",
    "    # create a list of keys for the current group\n",
    "    keys = [key for key in sorted_keys if key[:-2] == group]\n",
    "    # create a list of dataframes for the current group\n",
    "    dfs = [data[key] for key in keys]\n",
    "\n",
    "    # append the list of dataframes to the dictionary\n",
    "    group_dict[group] = dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(group_dict['7']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the full_dataset.csv file into a dataframe. Keep only the 'Dyad Number' and 'Truth/Lie' columns\n",
    "full_dataset = pd.read_csv('full_dataset.csv', usecols=['Dyad Number', 'Truth/Lie'])\n",
    "# delete the duplicates in the full_dataset dataframe based on the 'Dyad Number' column\n",
    "full_dataset = full_dataset.drop_duplicates(subset='Dyad Number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data['1_1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to transform the dataframes in a dictionary into a single 3d numpy array, structured as (n_samples, n_features, n_timepoints).\n",
    "# Use the keys of the dictionary, as integers, from the smallest to the largest, as the first dimension of the numpy array.\n",
    "#Use the columns of the dataframes as the second dimension of the numpy array.\n",
    "# Use the rows of the dataframes as the third dimension of the numpy array.\n",
    "\n",
    "def dict_to_array(data):\n",
    "    # get the number of keys\n",
    "    n_keys = len(list(data.keys()))\n",
    "    # get the number of columns\n",
    "    n_columns = data[list(data.keys())[0]].shape[1]\n",
    "    # get the number of rows\n",
    "    n_rows = data[list(data.keys())[0]].shape[0]\n",
    "    # create an empty numpy array\n",
    "    array = np.zeros((n_keys, n_columns, n_rows))\n",
    "    # create an empty list to store the groups from the keys\n",
    "    groups = np.array([])\n",
    "    # loop through the keys\n",
    "    for i in range(n_keys):\n",
    "        # get the key\n",
    "        key = list(data.keys())[i]\n",
    "        # get the group: the key except the last two characters\n",
    "        group = key[:-2]\n",
    "        # append the group to the list\n",
    "        groups = np.append(groups, int(group))\n",
    "        df = data[key]\n",
    "        # get the values of the dataframe\n",
    "        values = df.values\n",
    "        # store the values in the numpy array\n",
    "        array[i, :, :] = values.T\n",
    "    for element in groups:\n",
    "        element = int(element)\n",
    "    return array, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the dataframes in the dictionary into a single 3d numpy array\n",
    "print('working on X')\n",
    "X, groups_X = dict_to_array(data)\n",
    "print('working on X_A')\n",
    "X_A, groups_XA = dict_to_array(data_A)\n",
    "\n",
    "# create a label array, there 'Lie' is 0 and 'Truth' is 1\n",
    "y = full_dataset['Truth/Lie'].values\n",
    "y = np.where(y == 'Lie', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in groups_X:\n",
    "    element = int(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary using the \"Dyad Number\" column of the full_dataset dataframe as keys and the \"Truth/Lie\" column as values, where 'Lie' is 0 and 'Truth' is 1\n",
    "map = full_dataset.set_index('Dyad Number').to_dict()['Truth/Lie']\n",
    "# change each truth value in map to 1 and each lie value to 0\n",
    "for key in map.keys():\n",
    "    map[key] = 1 if map[key] == 'Truth' else 0\n",
    "\n",
    "# create a numpy array mapping each value in groups to the corresponding value in map\n",
    "y = np.array([map[group] for group in groups_X])\n",
    "y_a = np.array([map[group] for group in groups_XA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a canonical interval forest model\n",
    "cif = CanonicalIntervalForest(n_estimators=100, random_state=47, n_jobs=-1)\n",
    "\n",
    "# create a rocket model\n",
    "rocket = RocketClassifier(num_kernels=1000, random_state=47, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to perform the training using leave one out cross validation and to create the confusion matrix and the classification report\n",
    "def train_one_out(X, y, model):\n",
    "    # create a leave one out cross validation object\n",
    "    logo = LeaveOneGroupOut()\n",
    "    # create an empty list to store the predictions\n",
    "    predictions = []\n",
    "    # loop through the training and test sets\n",
    "    for i, (train_index, test_index) in enumerate(logo.split(X, y, groups=groups_X)):\n",
    "        print(f\"Fold {i}:\")\n",
    "        print(f\"  Train: index={train_index}\")\n",
    "        print(f\"  Test:  index={test_index}\")\n",
    "        # train the cif model\n",
    "        model.fit(X[train_index], y[train_index])\n",
    "        # the test set contains multiple samples. Produce a prediction for each sample\n",
    "        for j in range(len(test_index)):\n",
    "            # get the test sample\n",
    "            X_test = X[test_index][j]\n",
    "            # reshape X_test to 1, dimnsion 0, dimension 1\n",
    "            X_test = X_test.reshape(1, X_test.shape[0], X_test.shape[1])\n",
    "            y_pred = model.predict(X_test)\n",
    "            # store the prediction\n",
    "            predictions.append(y_pred)\n",
    "        print(len(predictions))\n",
    "    print('for loop done')\n",
    "    # create the confusion matrix\n",
    "    cm = confusion_matrix(y, predictions)\n",
    "    # create the classification report\n",
    "    cr = classification_report(y, predictions)\n",
    "    return cm, cr, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_rocket, cr_rocket, predictions_rocket = train_one_out(X, y, rocket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_cif, cr_cif, predictions_cif = train_one_out(X, y, cif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_rocket_A, cr_rocket_A, predictions_rocket_A = train_one_out(X_A, y, rocket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_cif_A, cr_cif_A, predictions_cif_A = train_one_out(X_A, y, cif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a file to store the results\n",
    "\n",
    "with open('results.txt', 'a') as f:\n",
    "    f.write('Rocket on merged_openface_out_cut\\n')\n",
    "    f.write('Confusion Matrix:\\n')\n",
    "    f.write(str(cm_rocket))\n",
    "    f.write('\\n')\n",
    "    f.write('Classification Report:\\n')\n",
    "    f.write(cr_rocket)\n",
    "    f.write('\\n')\n",
    "    f.write('Rocket on openface_out_A_cut\\n')\n",
    "    f.write('Confusion Matrix:\\n')\n",
    "    f.write(str(cm_rocket_A))\n",
    "    f.write('\\n')\n",
    "    f.write('Classification Report:\\n')\n",
    "    f.write(cr_rocket_A)\n",
    "    f.write('\\n')\n",
    "    f.write('CIF on merged_openface_out_cut\\n')\n",
    "    f.write('Confusion Matrix:\\n')\n",
    "    f.write(str(cm_cif))\n",
    "    f.write('\\n')\n",
    "    f.write('Classification Report:\\n')\n",
    "    f.write(cr_cif)\n",
    "    f.write('\\n')\n",
    "    f.write('CIF on openface_out_A_cut\\n')\n",
    "    f.write('Confusion Matrix:\\n')\n",
    "    f.write(str(cm_cif_A))\n",
    "    f.write('\\n')\n",
    "    f.write('Classification Report:\\n')\n",
    "    f.write(cr_cif_A)\n",
    "    f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
