{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the usual libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sktime.classification.interval_based import CanonicalIntervalForest\n",
    "from sktime.classification.kernel_based import RocketClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the .csv files in merged_openface_out as dataframes in a dictionary\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# get the current directory\n",
    "path = os.getcwd()\n",
    "\n",
    "print(path)\n",
    "# get the path to the directory with the csv files\n",
    "path = path + '/merged_openface_out'\n",
    "# get the list of files in the directory\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "# create an empty dictionary to store the dataframes\n",
    "data = {}\n",
    "# loop through the list of files\n",
    "for filename in all_files:\n",
    "    # get the name of the file\n",
    "    name = os.path.basename(filename)\n",
    "    # delete the .csv extension\n",
    "    name = name[:-4]\n",
    "    # read the file into a dataframe\n",
    "    df = pd.read_csv(filename, index_col='Unnamed: 0', header=0)\n",
    "    # drop the columns starting with timestamp\n",
    "    df = df.drop(df.filter(regex='timestamp').columns, axis=1)\n",
    "    # store the dataframe in the dictionary\n",
    "    data[name] = df\n",
    "\n",
    "# get the path to the directory with the csv files\n",
    "path = os.getcwd()\n",
    "\n",
    "path = path + '/openface_out_A'\n",
    "# get the list of files in the directory\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "print(all_files)\n",
    "# create an empty dictionary to store the dataframes\n",
    "data_A = {}\n",
    "# loop through the list of files\n",
    "for filename in all_files:\n",
    "    # get the name of the file\n",
    "    name = os.path.basename(filename)\n",
    "    # delete the .csv extension\n",
    "    name = name[:-4]\n",
    "    # read the file into a dataframe\n",
    "    df = pd.read_csv(filename, index_col='Unnamed: 0', header=0)\n",
    "    # drop the columns starting with timestamp\n",
    "    df = df.drop(df.filter(regex='timestamp').columns, axis=1)\n",
    "    # store the dataframe in the dictionary\n",
    "    data_A[name] = df\n",
    "\n",
    "# get the current directory\n",
    "path = os.getcwd()\n",
    "\n",
    "print(path)\n",
    "# get the path to the directory with the csv files\n",
    "path = path + '/merged_opensmile_out'\n",
    "# get the list of files in the directory\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "# create an empty dictionary to store the dataframes\n",
    "data_voice = {}\n",
    "# loop through the list of files\n",
    "for filename in all_files:\n",
    "    # get the name of the file\n",
    "    name = os.path.basename(filename)\n",
    "    # delete the .csv extension\n",
    "    name = name[:-4]\n",
    "    # read the file into a dataframe\n",
    "    df = pd.read_csv(filename, index_col='Unnamed: 0', header=0)\n",
    "    # drop the columns starting with timestamp\n",
    "    df = df.drop(df.filter(regex='timestamp').columns, axis=1)\n",
    "    # store the dataframe in the dictionary\n",
    "    data_voice[name] = df\n",
    "\n",
    "# get the current directory\n",
    "path = os.getcwd()\n",
    "\n",
    "print(path)\n",
    "# get the path to the directory with the csv files\n",
    "path = path + '/opensmile_out_A'\n",
    "# get the list of files in the directory\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "# create an empty dictionary to store the dataframes\n",
    "data_voice_A = {}\n",
    "# loop through the list of files\n",
    "for filename in all_files:\n",
    "    # get the name of the file\n",
    "    name = os.path.basename(filename)\n",
    "    # delete the .csv extension\n",
    "    name = name[:-4]\n",
    "    # read the file into a dataframe\n",
    "    df = pd.read_csv(filename, index_col='Unnamed: 0', header=0)\n",
    "    # drop the columns starting with timestamp\n",
    "    df = df.drop(df.filter(regex='timestamp').columns, axis=1)\n",
    "    # store the dataframe in the dictionary\n",
    "    data_voice_A[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes a dictionary of dataframes and returns a dictionary of dataframes in which each dataframe has been shortened by averaging the values in each column over a window of size window_size\n",
    "def average_over_window(data, window_size):\n",
    "    # create an empty dictionary to store the shortened dataframes\n",
    "    data_short = {}\n",
    "    # loop through the dataframes in the input dictionary\n",
    "    for key in data:\n",
    "        # get the dataframe\n",
    "        df = data[key]\n",
    "        # create an empty dataframe to store the shortened version\n",
    "        df_short = pd.DataFrame()\n",
    "        # loop through the columns in the dataframe\n",
    "        for col in df.columns:\n",
    "            # create an empty list to store the averaged values\n",
    "            avg = []\n",
    "            # loop through the values in the column\n",
    "            for i in range(0, len(df[col]), window_size):\n",
    "                # get the average of the values in the window\n",
    "                avg.append(np.mean(df[col][i:i+window_size]))\n",
    "            # add the averaged values to the shortened dataframe\n",
    "            df_short[col] = avg\n",
    "        # store the shortened dataframe in the output dictionary\n",
    "        data_short[key] = df_short\n",
    "    return data_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.series.paa import PAA\n",
    "\n",
    "paa = PAA(frames=10000)\n",
    "\n",
    "data_short = {}\n",
    "data_short_A = {}\n",
    "data_short_voice = {}\n",
    "data_short_voice_A = {}\n",
    "\n",
    "for key in data.keys():\n",
    "    data_short[key] = paa.fit_transform(data[key])\n",
    "    data_short_A[key] = paa.fit_transform(data_A[key])\n",
    "    data_short_voice[key] = paa.fit_transform(data_voice[key])\n",
    "    data_short_voice_A[key] = paa.fit_transform(data_voice_A[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframes in the three dictionaries as .csv files\n",
    "save_path = 'data_short/'\n",
    "save_path_A = 'data_short_A/'\n",
    "save_path_voice = 'data_short_voice/'\n",
    "save_path_voice_A = 'data_short_voice_A/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "if not os.path.exists(save_path_A):\n",
    "    os.makedirs(save_path_A)\n",
    "if not os.path.exists(save_path_voice):\n",
    "    os.makedirs(save_path_voice)\n",
    "if not os.path.exists(save_path_voice_A):\n",
    "    os.makedirs(save_path_voice_A)\n",
    "\n",
    "for key in data_short.keys():\n",
    "    data_short[key].to_csv(save_path + key + '.csv')\n",
    "    data_short_A[key].to_csv(save_path_A + key + '.csv')\n",
    "    data_short_voice[key].to_csv(save_path_voice + key + '.csv')\n",
    "    data_short_voice_A[key].to_csv(save_path_voice_A + key + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the full_dataset.csv file into a dataframe. Keep only the 'Dyad Number' and 'Truth/Lie' columns\n",
    "full_dataset = pd.read_csv('full_dataset.csv', usecols=['Dyad Number', 'Truth/Lie'])\n",
    "# delete the duplicates in the full_dataset dataframe based on the 'Dyad Number' column\n",
    "full_dataset = full_dataset.drop_duplicates(subset='Dyad Number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to transform the dataframes in a dictionary into a single 3d numpy array, structured as (n_samples, n_features, n_timepoints).\n",
    "# Use the keys of the dictionary, as integers, from the smallest to the largest, as the first dimension of the numpy array.\n",
    "#Use the columns of the dataframes as the second dimension of the numpy array.\n",
    "# Use the rows of the dataframes as the third dimension of the numpy array.\n",
    "\n",
    "def dict_to_array(data):\n",
    "    # get the keys of the dictionary\n",
    "    keys = list(data.keys())\n",
    "    # transform the keys into integers\n",
    "    keys = [int(key) for key in keys]\n",
    "    # sort the keys\n",
    "    keys.sort()\n",
    "    # transform the keys back into strings\n",
    "    keys = [str(key) for key in keys]\n",
    "    # print the keys\n",
    "    print(keys)\n",
    "    # get the number of keys\n",
    "    n_keys = len(keys)\n",
    "    # get the number of columns\n",
    "    n_columns = data[keys[0]].shape[1]\n",
    "    # get the number of rows\n",
    "    n_rows = data[keys[0]].shape[0]\n",
    "    # create an empty numpy array\n",
    "    array = np.zeros((n_keys, n_columns, n_rows))\n",
    "    # loop through the keys\n",
    "    for i in range(n_keys):\n",
    "        # get the key\n",
    "        key = keys[i]\n",
    "        # get the dataframe\n",
    "        df = data[key]\n",
    "        # get the values of the dataframe\n",
    "        values = df.values\n",
    "        # store the values in the numpy array\n",
    "        array[i, :, :] = values.T\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the dataframes in the dictionary into a single 3d numpy array\n",
    "X = dict_to_array(data_short)\n",
    "\n",
    "X_A = dict_to_array(data_short_A)\n",
    "\n",
    "X_Voice = dict_to_array(data_short_voice)\n",
    "\n",
    "X_Voice_A = dict_to_array(data_short_voice_A)\n",
    "\n",
    "# create a label array, there 'Lie' is 0 and 'Truth' is 1\n",
    "y = full_dataset['Truth/Lie'].values\n",
    "y = np.where(y == 'Lie', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to perform the training using leave one out cross validation and to create the confusion matrix and the classification report\n",
    "def train_one_out(X, y, model):\n",
    "    # create a leave one out cross validation object\n",
    "    loo = LeaveOneOut()\n",
    "    # create an empty list to store the predictions\n",
    "    predictions = []\n",
    "    # loop through the training and test sets\n",
    "    for i, (train_index, test_index) in enumerate(loo.split(X)):\n",
    "        print(f\"Fold {i}:\")\n",
    "        print(f\"  Train: index={train_index}\")\n",
    "        print(f\"  Test:  index={test_index}\")\n",
    "        # train the cif model\n",
    "        model.fit(X[train_index], y[train_index])\n",
    "        # produce a probability prediction\n",
    "        y_pred_test = model.predict(X[test_index])\n",
    "        # store the prediction\n",
    "        predictions.append(y_pred_test)\n",
    "    print(predictions)\n",
    "    # create the confusion matrix\n",
    "    cm = confusion_matrix(y, predictions)\n",
    "    # create the classification report\n",
    "    cr = classification_report(y, predictions)\n",
    "    return cm, cr, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a rocket model\n",
    "rocket = RocketClassifier(num_kernels=1000, random_state=47, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_merged = np.concatenate((X, X_Voice), axis=1)\n",
    "X_merged_a = np.concatenate((X_A, X_Voice), axis=1)\n",
    "X_merged_v = np.concatenate((X, X_Voice_A), axis=1)\n",
    "X_merged_va = np.concatenate((X_A, X_Voice_A), axis=1)\n",
    "\n",
    "cm_rocket, cr_rocket, predictions_rocket = train_one_out(X_merged, y, rocket)\n",
    "cm_rocket_a, cr_rocket_a, predictions_rocket_a = train_one_out(X_merged_a, y, rocket)\n",
    "cm_rocket_v, cr_rocket_v, predictions_rocket_v = train_one_out(X_merged_v, y, rocket)\n",
    "cm_rocket_va, cr_rocket_va, predictions_rocket_va = train_one_out(X_merged_va, y, rocket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.series.sax import SAX\n",
    "# apply the SAX transformation to the data, only to the last dimension of the numpy array\n",
    "sax = SAX(word_size=10000, alphabet_size=6)\n",
    "X = sax.fit_transform(X)\n",
    "X_A = sax.fit_transform(X_A)\n",
    "X_Voice = sax.fit_transform(X_Voice)\n",
    "X_Voice_A = sax.fit_transform(X_Voice_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_merged_sax = np.concatenate((X, X_Voice), axis=1)\n",
    "X_merged_a_sax = np.concatenate((X_A, X_Voice), axis=1)\n",
    "X_merged_v_sax = np.concatenate((X, X_Voice_A), axis=1)\n",
    "X_merged_va_sax = np.concatenate((X_A, X_Voice_A), axis=1)\n",
    "\n",
    "cm_rocket_sax, cr_rocket_sax, predictions_rocket_sax = train_one_out(X_merged_sax, y, rocket)\n",
    "cm_rocket_a_sax, cr_rocket_a_sax, predictions_rocket_a_sax = train_one_out(X_merged_a_sax, y, rocket)\n",
    "cm_rocket_v_sax, cr_rocket_v_sax, predictions_rocket_v_sax = train_one_out(X_merged_v_sax, y, rocket)\n",
    "cm_rocket_va_sax, cr_rocket_va_sax, predictions_rocket_va_sax = train_one_out(X_merged_va_sax, y, rocket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a file to store the results\n",
    "\n",
    "with open('results_ef2.txt', 'a') as f:\n",
    "    f.write('Rocket on merged_opensmile_out and merged_openface_out\\n')\n",
    "    f.write('Confusion Matrix:\\n')\n",
    "    f.write(str(cm_rocket))\n",
    "    f.write('\\n')\n",
    "    f.write('Classification Report:\\n')\n",
    "    f.write(cr_rocket)\n",
    "    f.write('\\n')\n",
    "    f.write('Rocket on merged_opensmile_out and openface_out_A\\n')\n",
    "    f.write('Confusion Matrix:\\n')\n",
    "    f.write(str(cm_rocket_a))\n",
    "    f.write('\\n')\n",
    "    f.write('Classification Report:\\n')\n",
    "    f.write(cr_rocket_a)\n",
    "    f.write('\\n')\n",
    "    f.write('Rocket on opensmile_A_out and merged_openface_out\\n')\n",
    "    f.write('Confusion Matrix:\\n')\n",
    "    f.write(str(cm_rocket_v))\n",
    "    f.write('\\n')\n",
    "    f.write('Classification Report:\\n')\n",
    "    f.write(cr_rocket_v)\n",
    "    f.write('\\n')\n",
    "    f.write('Rocket on opensmile_A_out and openface_out_A\\n')\n",
    "    f.write('Confusion Matrix:\\n')\n",
    "    f.write(str(cm_rocket_va))\n",
    "    f.write('\\n')\n",
    "    f.write('Classification Report:\\n')\n",
    "    f.write(cr_rocket_va)\n",
    "    f.write('\\n')\n",
    "\n",
    "    f.write('Rocket on merged_opensmile_out and merged_openface_out with SAX\\n')\n",
    "    f.write('Confusion Matrix:\\n')\n",
    "    f.write(str(cm_rocket_sax))\n",
    "    f.write('\\n')\n",
    "    f.write('Classification Report:\\n')\n",
    "    f.write(cr_rocket_sax)\n",
    "    f.write('\\n')\n",
    "    f.write('Rocket on merged_opensmile_out and openface_out_A with SAX\\n')\n",
    "    f.write('Confusion Matrix:\\n')\n",
    "    f.write(str(cm_rocket_a_sax))\n",
    "    f.write('\\n')\n",
    "    f.write('Classification Report:\\n')\n",
    "    f.write(cr_rocket_a_sax)\n",
    "    f.write('\\n')\n",
    "    f.write('Rocket on opensmile_A_out and merged_openface_out with SAX\\n')\n",
    "    f.write('Confusion Matrix:\\n')\n",
    "    f.write(str(cm_rocket_v_sax))\n",
    "    f.write('\\n')\n",
    "    f.write('Classification Report:\\n')\n",
    "    f.write(cr_rocket_v_sax)\n",
    "    f.write('\\n')\n",
    "    f.write('Rocket on opensmile_A_out and openface_out_A with SAX\\n')\n",
    "    f.write('Confusion Matrix:\\n')\n",
    "    f.write(str(cm_rocket_va_sax))\n",
    "    f.write('\\n')\n",
    "    f.write('Classification Report:\\n')\n",
    "    f.write(cr_rocket_va_sax)\n",
    "    f.write('\\n')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
